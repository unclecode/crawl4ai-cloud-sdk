{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl4AI Cloud — Interactive Tutorial\n",
    "\n",
    "Welcome to Crawl4AI Cloud. This notebook walks you through the platform's key features with **real, runnable examples**. Run each cell to see live results from the API.\n",
    "\n",
    "**What you'll learn:**\n",
    "- **One-Shot Extraction** — Generate a schema once with AI, reuse it for free\n",
    "- **LLM Extraction** — Extract structured data from unstructured pages\n",
    "- **Sync & Batch Crawl** — Quick tests with immediate results\n",
    "- **Async Crawl** — Scale to hundreds of URLs with job queues\n",
    "- **Deep Crawl** — Discover and crawl entire sites\n",
    "- **Advanced Configuration** — Content filtering, wait conditions, and more\n",
    "\n",
    "---\n",
    "\n",
    "### Setup\n",
    "\n",
    "Replace `YOUR_API_KEY` below with your real key. Get one from [crawl4ai.com/keys](https://crawl4ai.com/keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q crawl4ai-cloud-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from crawl4ai_cloud import AsyncWebCrawler, CrawlerRunConfig\n",
    "\n",
    "API_KEY = \"YOUR_API_KEY\"  # <-- Replace with your key\n",
    "crawler = AsyncWebCrawler(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. One-Shot Extraction\n",
    "\n",
    "Most web pages have repeating patterns — product listings, search results, article feeds. One-shot extraction lets you pull structured data from these pages using a schema-based approach:\n",
    "\n",
    "1. **Generate a schema once with AI** — it analyzes the HTML and produces CSS selectors\n",
    "2. **Apply the schema to any page** with the same structure — no AI costs, just fast pattern matching\n",
    "\n",
    "This is the most cost-effective extraction method at scale. AI runs once, then you reuse the schema across unlimited pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Generate Schema\n",
    "\n",
    "Point the schema generator at a page and describe what you want in natural language. AI analyzes the HTML and produces a set of CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = await crawler.generate_schema(\n",
    "    urls=[\"https://books.toscrape.com\"],\n",
    "    query=\"Extract all book titles, prices, and ratings\"\n",
    ")\n",
    "print(json.dumps(schema.schema, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a schema — a set of CSS selectors. AI figured out the page structure, but **applying the schema costs nothing**. You can also edit the schema by hand to fine-tune selectors or add fields.\n",
    "\n",
    "### 1b. Apply Schema\n",
    "\n",
    "Now use that schema to extract data. This is pure CSS pattern matching — fast and free of AI costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await crawler.run(\n",
    "    \"https://books.toscrape.com\",\n",
    "    config=CrawlerRunConfig(\n",
    "        extraction_strategy={\n",
    "            \"type\": \"json_css\",\n",
    "            \"schema\": schema.to_dict()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "extracted = result.extracted_content\n",
    "if isinstance(extracted, str):\n",
    "    extracted = json.loads(extracted)\n",
    "print(f\"Extracted {len(extracted)} items:\")\n",
    "print(json.dumps(extracted[:3], indent=2))\n",
    "print(f\"... and {len(extracted) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the speed — no AI call, just CSS selectors. Now use the **same schema** on page 2:\n",
    "\n",
    "### 1c. Reuse on Another Page\n",
    "\n",
    "Same schema, different page — same speed. This is the power of one-shot extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = await crawler.run(\n",
    "    \"https://books.toscrape.com/catalogue/page-2.html\",\n",
    "    config=CrawlerRunConfig(\n",
    "        extraction_strategy={\n",
    "            \"type\": \"json_css\",\n",
    "            \"schema\": schema.to_dict()\n",
    "        }\n",
    "    )\n",
    ")\n",
    "extracted2 = result2.extracted_content\n",
    "if isinstance(extracted2, str):\n",
    "    extracted2 = json.loads(extracted2)\n",
    "print(f\"Page 2 — extracted {len(extracted2)} items:\")\n",
    "print(json.dumps(extracted2[:3], indent=2))\n",
    "print(f\"... and {len(extracted2) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One schema, unlimited pages.** When the site redesigns, regenerate the schema once. That's the one-shot pattern.\n",
    "\n",
    "---\n",
    "## 2. LLM Extraction\n",
    "\n",
    "Schema-based extraction handles repeating patterns. But what about unstructured content — a recipe's ingredients, an article's key takeaways, specific facts from prose?\n",
    "\n",
    "LLM extraction works here. AI reads the page and extracts exactly what you ask for.\n",
    "\n",
    "You can use our managed AI model or bring your own API key. We're continuously fine-tuning a specialized model for schema generation and JSON extraction — this is an active area of development.\n",
    "\n",
    "**When to use which:**\n",
    "- **Schema-based (one-shot)** — catalogs, listings, repeating patterns (fast, no AI cost at scale)\n",
    "- **LLM extraction** — individual pages with unique, unstructured content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = await crawler.run(\n",
    "    \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n",
    "    config=CrawlerRunConfig(\n",
    "        extraction_strategy={\n",
    "            \"type\": \"llm\",\n",
    "            \"instruction\": \"Extract the book title, price, description, and availability status\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "llm_content = llm_result.extracted_content\n",
    "if isinstance(llm_content, str):\n",
    "    try:\n",
    "        llm_content = json.loads(llm_content)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "print(json.dumps(llm_content, indent=2) if isinstance(llm_content, (dict, list)) else llm_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Sync & Batch Crawl\n",
    "\n",
    "Everything above uses crawling under the hood. Sync and batch crawls are your tools for **quick tests and immediate results** — up to 10 URLs at a time.\n",
    "\n",
    "They're great for experimentation before moving to async or deep crawl for real workloads.\n",
    "\n",
    "### 3a. Sync Crawl\n",
    "\n",
    "Crawl a single URL and get markdown, metadata, and links back instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync = await crawler.run(\"https://example.com\")\n",
    "print(\"Markdown output:\")\n",
    "print(sync.markdown.raw_markdown[:500])\n",
    "print(f\"\\nLinks found: {len(sync.links.get('external', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Batch Crawl\n",
    "\n",
    "Crawl multiple URLs in one request using `run_many()`. With `wait=True`, results come back together when all URLs are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = await crawler.run_many(\n",
    "    urls=[\n",
    "        \"https://example.com\",\n",
    "        \"https://httpbin.org/html\",\n",
    "        \"https://books.toscrape.com\"\n",
    "    ],\n",
    "    wait=True\n",
    ")\n",
    "print(f\"Completed: {job.progress.completed}/{job.progress.total}\")\n",
    "print(f\"Failed: {job.progress.failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sync and batch are great for experimentation. For real workloads — dozens to thousands of URLs — use async crawling or deep crawl.\n",
    "\n",
    "---\n",
    "## 4. Async Crawl\n",
    "\n",
    "Async crawl handles **real-world scale**. Submit a job with up to 100 URLs, get a job ID back immediately, then poll for results. No connection timeouts, no waiting.\n",
    "\n",
    "### 4a. Submit a Job\n",
    "\n",
    "Without `wait=True`, `run_many()` returns immediately with a job ID. The crawling happens in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = await crawler.run_many(\n",
    "    urls=[\n",
    "        \"https://example.com\",\n",
    "        \"https://books.toscrape.com\",\n",
    "        \"https://httpbin.org/html\",\n",
    "        \"https://quotes.toscrape.com\",\n",
    "        \"https://webscraper.io/test-sites/e-commerce/allinone\"\n",
    "    ]\n",
    ")\n",
    "print(f\"Job ID: {job.id}\")\n",
    "print(f\"Status: {job.status}\")\n",
    "print(f\"URLs: {job.urls_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Poll for Results\n",
    "\n",
    "Use `wait_job()` to poll until the job completes. You can also use `get_job()` for a single status check, or set up a webhook for push notifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed = await crawler.wait_job(job.id, poll_interval=2.0, timeout=300)\n",
    "print(f\"Status: {completed.status}\")\n",
    "print(f\"Progress: {completed.progress.completed}/{completed.progress.total}\")\n",
    "print(f\"Failed: {completed.progress.failed}\")\n",
    "\n",
    "if completed.is_complete:\n",
    "    url = await crawler.download_url(completed.id)\n",
    "    print(f\"\\nDownload results: {url[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Deep Crawl\n",
    "\n",
    "Deep crawl is how you crawl **an entire site**. It works in two stages:\n",
    "\n",
    "1. **Scan** — discovers all URLs using sitemaps, link analysis, and site structure\n",
    "2. **Extract** — crawls the discovered pages with your configuration\n",
    "\n",
    "Most tools discover pages one at a time through link traversal. Our scan phase finds all URLs upfront — you see the full map before crawling a single page.\n",
    "\n",
    "### Scanning Strategies\n",
    "\n",
    "- **`map`** (sitemap-based) — reads sitemaps and available internet data. Fast and reliable for established, well-indexed sites.\n",
    "- **`bfs`** (breadth-first) — traverses links level by level. Use when sitemaps aren't available (new sites, poor indexing).\n",
    "- **`dfs`** (depth-first) — follows links deep before going wide.\n",
    "- **`best_first`** — priority-based with keyword scoring.\n",
    "\n",
    "You only need to scan once. The URL map is cached. Re-extract anytime without re-scanning.\n",
    "\n",
    "### 5a. Scan with Map Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan = await crawler.deep_crawl(\n",
    "    \"https://docs.crawl4ai.com\",\n",
    "    strategy=\"map\",\n",
    "    scan_only=True,\n",
    "    wait=True,\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"Discovered {scan.discovered_count} URLs\")\n",
    "print(f\"\\nSample URLs:\")\n",
    "if scan.urls:\n",
    "    for u in scan.urls[:10]:\n",
    "        print(f\"  {u.url}\")\n",
    "    if len(scan.urls) > 10:\n",
    "        print(f\"  ... and {len(scan.urls) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Scan with BFS (Link Traversal)\n",
    "\n",
    "For sites without good sitemaps, use BFS. It traverses links level by level to discover pages. Slower than sitemap-based scanning but works universally.\n",
    "\n",
    "> **Tip:** You can combine `include_patterns` and `exclude_patterns` to filter which URLs get discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs_scan = await crawler.deep_crawl(\n",
    "    \"https://books.toscrape.com\",\n",
    "    strategy=\"bfs\",\n",
    "    max_depth=2,\n",
    "    max_urls=20,\n",
    "    scan_only=True,\n",
    "    wait=True,\n",
    "    timeout=120\n",
    ")\n",
    "print(f\"BFS discovered {bfs_scan.discovered_count} URLs (max_depth=2, max_urls=20)\")\n",
    "if bfs_scan.urls:\n",
    "    for u in bfs_scan.urls[:5]:\n",
    "        print(f\"  {u.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Advanced Configuration\n",
    "\n",
    "Every crawl accepts a `CrawlerRunConfig` — the same configuration object from the open-source [Crawl4AI](https://github.com/unclecode/crawl4ai) library. It controls:\n",
    "\n",
    "- **JavaScript execution** — run JS code, wait for elements, click buttons\n",
    "- **Wait conditions** — wait for network idle, specific selectors, or timeouts\n",
    "- **Content filtering** — BM25 relevance scoring to return only relevant sections\n",
    "- **Screenshots & PDFs** — capture visual snapshots\n",
    "- **Element removal** — strip overlays, popups, cookie banners\n",
    "\n",
    "Here's an example combining crawling with content filtering. The `content_filter` uses BM25 to return only sections relevant to your query — no AI needed, great for building focused context.\n",
    "\n",
    "> **Tip:** Full `CrawlerRunConfig` reference at [docs.crawl4ai.com](https://docs.crawl4ai.com). Join the [Discord](https://discord.gg/crawl4ai) for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = await crawler.run(\n",
    "    \"https://docs.crawl4ai.com\",\n",
    "    config=CrawlerRunConfig(\n",
    "        wait_until=\"networkidle\",\n",
    "        page_timeout=30000,\n",
    "        remove_overlay_elements=True,\n",
    "        content_filter={\n",
    "            \"type\": \"bm25\",\n",
    "            \"query\": \"authentication setup\",\n",
    "            \"threshold\": 1.0\n",
    "        }\n",
    "    )\n",
    ")\n",
    "md = adv.markdown.raw_markdown if adv.markdown else \"\"\n",
    "if md:\n",
    "    print(f\"Filtered markdown ({len(md)} chars — only authentication-related sections):\")\n",
    "    print(md[:1000])\n",
    "else:\n",
    "    print(\"No sections matched the filter (try a broader query or lower threshold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You've seen the core features. Here's where to go from here:\n",
    "\n",
    "- **API Reference** — Full endpoint documentation at [docs.crawl4ai.com](https://docs.crawl4ai.com)\n",
    "- **SDKs** — [Python](https://github.com/unclecode/crawl4ai) · [Node.js](https://github.com/unclecode/crawl4ai-js) · [Go](https://github.com/unclecode/crawl4ai-go)\n",
    "- **Community** — Join us on [Discord](https://discord.gg/crawl4ai) — we're active and help\n",
    "- **Open Source** — [github.com/unclecode/crawl4ai](https://github.com/unclecode/crawl4ai) — 60k+ stars\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Method | Use Case |\n",
    "|--------|----------|\n",
    "| `crawler.run(url)` | Single URL, immediate result |\n",
    "| `crawler.run_many(urls, wait=True)` | Multiple URLs, wait for all |\n",
    "| `crawler.run_many(urls)` | Fire-and-forget, poll with `wait_job()` |\n",
    "| `crawler.deep_crawl(url, strategy=\"map\")` | Discover all URLs on a site |\n",
    "| `crawler.generate_schema(urls, query)` | Generate CSS extraction schema with AI |\n",
    "| `CrawlerRunConfig(extraction_strategy={...})` | Configure extraction, filtering, JS, etc. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await crawler.close()\n",
    "print(\"Done! All examples completed.\")"
   ]
  }
 ]
}
